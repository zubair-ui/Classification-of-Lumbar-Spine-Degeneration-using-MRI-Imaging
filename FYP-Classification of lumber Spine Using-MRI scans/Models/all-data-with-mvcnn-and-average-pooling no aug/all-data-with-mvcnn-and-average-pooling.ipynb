{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64cc2051",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-21T13:43:06.966650Z",
     "iopub.status.busy": "2025-06-21T13:43:06.966405Z",
     "iopub.status.idle": "2025-06-21T19:39:46.684980Z",
     "shell.execute_reply": "2025-06-21T19:39:46.683433Z"
    },
    "papermill": {
     "duration": 21399.730219,
     "end_time": "2025-06-21T19:39:46.693957",
     "exception": false,
     "start_time": "2025-06-21T13:43:06.963738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 13:43:11.344960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750513391.602766      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750513391.679299      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "I0000 00:00:1750513406.544929      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1750513572.398804      59 service.cc:148] XLA service 0x7f214c002350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1750513572.399894      59 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1750513581.014150      59 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "E0000 00:00:1750513590.392451      59 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1750513590.589877      59 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "I0000 00:00:1750513610.426135      59 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33s/step - accuracy: 0.6629 - loss: 0.8180 \n",
      "Epoch 1: val_loss improved from inf to 0.73392, saving model to best_model_Adam.keras\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2146s\u001b[0m 42s/step - accuracy: 0.6641 - loss: 0.8159 - val_accuracy: 0.7694 - val_loss: 0.7339 - learning_rate: 1.0000e-04\n",
      "Epoch 2/15\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23s/step - accuracy: 0.7793 - loss: 0.5561 \n",
      "Epoch 2: val_loss improved from 0.73392 to 0.71864, saving model to best_model_Adam.keras\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1330s\u001b[0m 27s/step - accuracy: 0.7792 - loss: 0.5563 - val_accuracy: 0.7654 - val_loss: 0.7186 - learning_rate: 1.0000e-04\n",
      "Epoch 3/15\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21s/step - accuracy: 0.7785 - loss: 0.5298 \n",
      "Epoch 3: val_loss improved from 0.71864 to 0.68468, saving model to best_model_Adam.keras\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1316s\u001b[0m 27s/step - accuracy: 0.7786 - loss: 0.5296 - val_accuracy: 0.7674 - val_loss: 0.6847 - learning_rate: 1.0000e-04\n",
      "Epoch 4/15\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22s/step - accuracy: 0.7998 - loss: 0.4768 \n",
      "Epoch 4: val_loss improved from 0.68468 to 0.64258, saving model to best_model_Adam.keras\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1292s\u001b[0m 27s/step - accuracy: 0.7998 - loss: 0.4768 - val_accuracy: 0.7685 - val_loss: 0.6426 - learning_rate: 1.0000e-04\n",
      "Epoch 5/15\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21s/step - accuracy: 0.8115 - loss: 0.4510 \n",
      "Epoch 5: val_loss did not improve from 0.64258\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1373s\u001b[0m 28s/step - accuracy: 0.8116 - loss: 0.4508 - val_accuracy: 0.7680 - val_loss: 0.6443 - learning_rate: 1.0000e-04\n",
      "Epoch 6/15\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21s/step - accuracy: 0.8368 - loss: 0.3969 \n",
      "Epoch 6: val_loss improved from 0.64258 to 0.64013, saving model to best_model_Adam.keras\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1251s\u001b[0m 26s/step - accuracy: 0.8367 - loss: 0.3970 - val_accuracy: 0.7671 - val_loss: 0.6401 - learning_rate: 1.0000e-04\n",
      "Epoch 7/15\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21s/step - accuracy: 0.8442 - loss: 0.3811 \n",
      "Epoch 7: val_loss improved from 0.64013 to 0.63314, saving model to best_model_Adam.keras\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1326s\u001b[0m 27s/step - accuracy: 0.8442 - loss: 0.3810 - val_accuracy: 0.7635 - val_loss: 0.6331 - learning_rate: 1.0000e-04\n",
      "Epoch 8/15\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22s/step - accuracy: 0.8635 - loss: 0.3390 \n",
      "Epoch 8: val_loss did not improve from 0.63314\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1310s\u001b[0m 27s/step - accuracy: 0.8634 - loss: 0.3391 - val_accuracy: 0.7683 - val_loss: 0.6440 - learning_rate: 1.0000e-04\n",
      "Epoch 9/15\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22s/step - accuracy: 0.8763 - loss: 0.3142 \n",
      "Epoch 9: val_loss improved from 0.63314 to 0.61408, saving model to best_model_Adam.keras\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1455s\u001b[0m 30s/step - accuracy: 0.8762 - loss: 0.3143 - val_accuracy: 0.7636 - val_loss: 0.6141 - learning_rate: 1.0000e-04\n",
      "Epoch 10/15\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25s/step - accuracy: 0.8847 - loss: 0.2883 \n",
      "Epoch 10: val_loss did not improve from 0.61408\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1476s\u001b[0m 30s/step - accuracy: 0.8847 - loss: 0.2883 - val_accuracy: 0.7599 - val_loss: 0.6360 - learning_rate: 1.0000e-04\n",
      "Epoch 11/15\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23s/step - accuracy: 0.8950 - loss: 0.2689 \n",
      "Epoch 11: val_loss did not improve from 0.61408\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1430s\u001b[0m 29s/step - accuracy: 0.8951 - loss: 0.2688 - val_accuracy: 0.7572 - val_loss: 0.6207 - learning_rate: 1.0000e-04\n",
      "Epoch 12/15\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23s/step - accuracy: 0.9035 - loss: 0.2504 \n",
      "Epoch 12: val_loss did not improve from 0.61408\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1403s\u001b[0m 29s/step - accuracy: 0.9037 - loss: 0.2501 - val_accuracy: 0.7608 - val_loss: 0.6238 - learning_rate: 3.0000e-05\n",
      "Epoch 13/15\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23s/step - accuracy: 0.9218 - loss: 0.2150 \n",
      "Epoch 13: val_loss did not improve from 0.61408\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1438s\u001b[0m 30s/step - accuracy: 0.9218 - loss: 0.2150 - val_accuracy: 0.7649 - val_loss: 0.6247 - learning_rate: 3.0000e-05\n",
      "Epoch 14/15\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23s/step - accuracy: 0.9269 - loss: 0.2036 \n",
      "Epoch 14: val_loss did not improve from 0.61408\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1364s\u001b[0m 28s/step - accuracy: 0.9269 - loss: 0.2037 - val_accuracy: 0.7657 - val_loss: 0.6353 - learning_rate: 9.0000e-06\n",
      "Epoch 15/15\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22s/step - accuracy: 0.9272 - loss: 0.2013 \n",
      "Epoch 15: val_loss did not improve from 0.61408\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1394s\u001b[0m 29s/step - accuracy: 0.9272 - loss: 0.2014 - val_accuracy: 0.7652 - val_loss: 0.6397 - learning_rate: 9.0000e-06\n",
      "Restoring model weights from the end of the best epoch: 9.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import cv2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Configs\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "DATA_DIR = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/'\n",
    "\n",
    "# Load CSVs\n",
    "train_df = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train.csv')\n",
    "series_desc_df = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv')\n",
    "\n",
    "# Labels\n",
    "label_cols = train_df.columns[1:]\n",
    "label_map = {'Normal/Mild': 0, 'Moderate': 1, 'Severe': 2}\n",
    "\n",
    "def encode_labels(row):\n",
    "    return [label_map.get(row[col], 0) for col in label_cols]\n",
    "train_df['encoded_labels'] = train_df.apply(encode_labels, axis=1)\n",
    "\n",
    "def get_max_severity(encoded_labels):\n",
    "    return max(encoded_labels)\n",
    "train_df['max_severity'] = train_df['encoded_labels'].apply(get_max_severity)\n",
    "\n",
    "\n",
    "# Series IDs per study\n",
    "def get_series_ids(study_id):\n",
    "    sub_df = series_desc_df[series_desc_df['study_id'] == study_id]\n",
    "    views = {'Sagittal T1': None, 'Sagittal T2/STIR': None, 'Axial T2': None}\n",
    "    for view in views:\n",
    "        found = sub_df[sub_df['series_description'].str.contains(view, case=False)]\n",
    "        if not found.empty:\n",
    "            views[view] = found.iloc[0]['series_id']\n",
    "    return views\n",
    "\n",
    "# Load DICOM without augmentation\n",
    "def load_dicom_image(path):\n",
    "    dcm = pydicom.dcmread(path)\n",
    "    img = dcm.pixel_array.astype(np.float32)\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    img = img / np.max(img)\n",
    "    return img\n",
    "\n",
    "# Load images for a study and average them over all available images per view\n",
    "def load_study_images(study_id):\n",
    "    views = get_series_ids(study_id)\n",
    "    images = []\n",
    "    for view in ['Sagittal T1', 'Sagittal T2/STIR', 'Axial T2']:\n",
    "        series_id = views[view]\n",
    "        if pd.isna(series_id):\n",
    "            images.append(np.zeros((IMG_SIZE, IMG_SIZE, 3)))  # No image, so append blank image\n",
    "        else:\n",
    "            series_path = os.path.join(DATA_DIR, str(study_id), str(series_id))\n",
    "            instances = sorted(os.listdir(series_path))\n",
    "            view_images = []\n",
    "            for instance in instances:\n",
    "                img_path = os.path.join(series_path, instance)\n",
    "                view_images.append(load_dicom_image(img_path))  # Load all images for this view\n",
    "            if view_images:\n",
    "                # Average pooling over all images in this view\n",
    "                avg_image = np.mean(view_images, axis=0)\n",
    "                images.append(avg_image)\n",
    "            else:\n",
    "                images.append(np.zeros((IMG_SIZE, IMG_SIZE, 3)))  # If no images, append blank image\n",
    "    return images\n",
    "\n",
    "# Data generator without augmentation\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, batch_size=BATCH_SIZE, shuffle=True):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.df))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.df) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_ids = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        batch_df = self.df.iloc[batch_ids]\n",
    "        X1, X2, X3, y = [], [], [], []\n",
    "        for _, row in batch_df.iterrows():\n",
    "            imgs = load_study_images(row['study_id'])\n",
    "            # Append the average pooled image for each view\n",
    "            X1.append(np.array(imgs[0]))  # Sagittal T1\n",
    "            X2.append(np.array(imgs[1]))  # Sagittal T2/STIR\n",
    "            X3.append(np.array(imgs[2]))  # Axial T2\n",
    "            y.append(row['encoded_labels'])\n",
    "        return (np.array(X1), np.array(X2), np.array(X3)), to_categorical(np.array(y), num_classes=3)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "# Backbone creation with fine-tuning last 50 layers\n",
    "def create_backbone():\n",
    "    base = applications.MobileNetV2(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3), pooling='avg')\n",
    "    for layer in base.layers[-50:]:\n",
    "        layer.trainable = True\n",
    "    return base\n",
    "\n",
    "# Build Multi-View CNN\n",
    "def build_mvcnn():\n",
    "    input1 = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    input2 = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    input3 = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    backbone = create_backbone()\n",
    "    feat1 = backbone(input1)\n",
    "    feat2 = backbone(input2)\n",
    "    feat3 = backbone(input3)\n",
    "    merged = layers.Concatenate()([feat1, feat2, feat3])\n",
    "    x = layers.Dense(512, activation='relu')(merged)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output = layers.Dense(len(label_cols)*3, activation='softmax')(x)\n",
    "    output = layers.Reshape((len(label_cols), 3))(output)\n",
    "    model = models.Model(inputs=[input1, input2, input3], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Train/Validation Split\n",
    "train_ids, val_ids = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['max_severity'])\n",
    " \n",
    "\n",
    "train_gen = DataGenerator(train_ids)\n",
    "val_gen = DataGenerator(val_ids)\n",
    "\n",
    "# Build and train model\n",
    "model = build_mvcnn()\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('best_model_Adam.keras', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "# Train\n",
    "model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "model.save('mvc_MobileNetV2_no_aug_400k.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8561470,
     "sourceId": 71549,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21407.248449,
   "end_time": "2025-06-21T19:39:49.689964",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-21T13:43:02.441515",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
