{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc2c078a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-21T04:22:50.098766Z",
     "iopub.status.busy": "2025-06-21T04:22:50.098483Z",
     "iopub.status.idle": "2025-06-21T04:51:29.331247Z",
     "shell.execute_reply": "2025-06-21T04:51:29.330400Z"
    },
    "papermill": {
     "duration": 1719.237678,
     "end_time": "2025-06-21T04:51:29.332864",
     "exception": false,
     "start_time": "2025-06-21T04:22:50.095186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 04:22:53.962812: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750479774.172670      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750479774.230321      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training set: 1580\n",
      "Number of images in the validation set: 395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750479788.633330      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1750479855.700875      58 service.cc:148] XLA service 0x789a88005220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1750479855.701649      58 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1750479862.060597      58 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/197\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:34:57\u001b[0m 103s/step - accuracy: 0.3150 - loss: 1.4039"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750479895.017858      58 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 951ms/step - accuracy: 0.6896 - loss: 0.8069 - val_accuracy: 0.7644 - val_loss: 0.7297\n",
      "Epoch 2/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 301ms/step - accuracy: 0.7698 - loss: 0.6435 - val_accuracy: 0.7708 - val_loss: 0.6758\n",
      "Epoch 3/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 302ms/step - accuracy: 0.7801 - loss: 0.6130 - val_accuracy: 0.7619 - val_loss: 0.6456\n",
      "Epoch 4/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 303ms/step - accuracy: 0.7890 - loss: 0.5890 - val_accuracy: 0.7678 - val_loss: 0.6499\n",
      "Epoch 5/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 310ms/step - accuracy: 0.7946 - loss: 0.5737 - val_accuracy: 0.7622 - val_loss: 0.6479\n",
      "Epoch 6/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 299ms/step - accuracy: 0.8079 - loss: 0.5545 - val_accuracy: 0.7674 - val_loss: 0.6541\n",
      "Epoch 7/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 295ms/step - accuracy: 0.8126 - loss: 0.5427 - val_accuracy: 0.7464 - val_loss: 0.6561\n",
      "Epoch 8/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 300ms/step - accuracy: 0.8215 - loss: 0.5264 - val_accuracy: 0.7511 - val_loss: 0.6526\n",
      "Epoch 9/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 295ms/step - accuracy: 0.8334 - loss: 0.5050 - val_accuracy: 0.7700 - val_loss: 0.6406\n",
      "Epoch 10/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 289ms/step - accuracy: 0.8477 - loss: 0.4782 - val_accuracy: 0.7597 - val_loss: 0.6439\n",
      "Epoch 11/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 292ms/step - accuracy: 0.8540 - loss: 0.4720 - val_accuracy: 0.7619 - val_loss: 0.6744\n",
      "Epoch 12/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 294ms/step - accuracy: 0.8659 - loss: 0.4477 - val_accuracy: 0.7662 - val_loss: 0.7053\n",
      "Epoch 13/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 308ms/step - accuracy: 0.8711 - loss: 0.4413 - val_accuracy: 0.7571 - val_loss: 0.6812\n",
      "Epoch 14/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 295ms/step - accuracy: 0.8820 - loss: 0.4238 - val_accuracy: 0.7517 - val_loss: 0.6890\n",
      "Epoch 15/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 292ms/step - accuracy: 0.8891 - loss: 0.4099 - val_accuracy: 0.7486 - val_loss: 0.6998\n",
      "Epoch 16/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 291ms/step - accuracy: 0.8965 - loss: 0.3959 - val_accuracy: 0.7511 - val_loss: 0.6958\n",
      "Epoch 17/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 296ms/step - accuracy: 0.9037 - loss: 0.3857 - val_accuracy: 0.7288 - val_loss: 0.7304\n",
      "Epoch 18/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 296ms/step - accuracy: 0.9068 - loss: 0.3761 - val_accuracy: 0.7373 - val_loss: 0.7286\n",
      "Epoch 19/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 294ms/step - accuracy: 0.9187 - loss: 0.3571 - val_accuracy: 0.7521 - val_loss: 0.7094\n",
      "Epoch 20/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 295ms/step - accuracy: 0.9125 - loss: 0.3652 - val_accuracy: 0.7305 - val_loss: 0.7357\n",
      "Epoch 21/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 313ms/step - accuracy: 0.9221 - loss: 0.3483 - val_accuracy: 0.7405 - val_loss: 0.7426\n",
      "Epoch 22/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 299ms/step - accuracy: 0.9289 - loss: 0.3357 - val_accuracy: 0.7581 - val_loss: 0.7310\n",
      "Epoch 23/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 295ms/step - accuracy: 0.9335 - loss: 0.3297 - val_accuracy: 0.7447 - val_loss: 0.7498\n",
      "Epoch 24/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 296ms/step - accuracy: 0.9389 - loss: 0.3187 - val_accuracy: 0.7441 - val_loss: 0.7837\n",
      "Epoch 25/25\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 291ms/step - accuracy: 0.9415 - loss: 0.3145 - val_accuracy: 0.7586 - val_loss: 0.7799\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import cv2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Configs\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 25\n",
    "DATA_DIR = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/'\n",
    "\n",
    "# Load CSVs\n",
    "train_df = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train.csv')\n",
    "series_desc_df = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv')\n",
    "\n",
    "# Labels\n",
    "label_cols = train_df.columns[1:]\n",
    "label_map = {'Normal/Mild': 0, 'Moderate': 1, 'Severe': 2}\n",
    "\n",
    "def encode_labels(row):\n",
    "    return [label_map.get(row[col], 0) for col in label_cols]\n",
    "train_df['encoded_labels'] = train_df.apply(encode_labels, axis=1)\n",
    "\n",
    "def get_max_severity(encoded_labels):\n",
    "    return max(encoded_labels)\n",
    "train_df['max_severity'] = train_df['encoded_labels'].apply(get_max_severity)\n",
    "  \n",
    "\n",
    "# Series IDs per study\n",
    "def get_series_ids(study_id):\n",
    "    sub_df = series_desc_df[series_desc_df['study_id'] == study_id]\n",
    "    views = {'Sagittal T1': None, 'Sagittal T2/STIR': None, 'Axial T2': None}\n",
    "    for view in views:\n",
    "        found = sub_df[sub_df['series_description'].str.contains(view, case=False)]\n",
    "        if not found.empty:\n",
    "            views[view] = found.iloc[0]['series_id']\n",
    "    return views\n",
    "\n",
    "# Load DICOM without augmentation\n",
    "def load_dicom_image(path):\n",
    "    dcm = pydicom.dcmread(path)\n",
    "    img = dcm.pixel_array.astype(np.float32)\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    img = img / np.max(img)\n",
    "    return img\n",
    "\n",
    "# Load images for a study\n",
    "def load_study_images(study_id):\n",
    "    views = get_series_ids(study_id)\n",
    "    images = []\n",
    "    for view in ['Sagittal T1', 'Sagittal T2/STIR', 'Axial T2']:\n",
    "        series_id = views[view]\n",
    "        if pd.isna(series_id):\n",
    "            images.append(np.zeros((IMG_SIZE, IMG_SIZE, 3)))\n",
    "        else:\n",
    "            series_path = os.path.join(DATA_DIR, str(study_id), str(series_id))\n",
    "            instances = sorted(os.listdir(series_path))\n",
    "            if instances:\n",
    "                img_path = os.path.join(series_path, instances[len(instances)//2])\n",
    "                images.append(load_dicom_image(img_path))\n",
    "            else:\n",
    "                images.append(np.zeros((IMG_SIZE, IMG_SIZE, 3)))\n",
    "    return images\n",
    "\n",
    "# Data generator without augmentation\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, batch_size=BATCH_SIZE, shuffle=True):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.df))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.df) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_ids = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        batch_df = self.df.iloc[batch_ids]\n",
    "        X1, X2, X3, y = [], [], [], []\n",
    "        for _, row in batch_df.iterrows():\n",
    "            imgs = load_study_images(row['study_id'])\n",
    "            X1.append(imgs[0])\n",
    "            X2.append(imgs[1])\n",
    "            X3.append(imgs[2])\n",
    "            y.append(row['encoded_labels'])\n",
    "        return (np.array(X1), np.array(X2), np.array(X3)), to_categorical(np.array(y), num_classes=3)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "# Backbone creation with fine-tuning last 65 layers\n",
    "def create_backbone():\n",
    "    base = applications.MobileNetV2(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3), pooling='avg')\n",
    "    for layer in base.layers[-65:]:\n",
    "        layer.trainable = True\n",
    "    return base\n",
    "\n",
    "# Build Multi-View CNN\n",
    "def build_mvcnn():\n",
    "    input1 = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    input2 = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    input3 = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    backbone = create_backbone()\n",
    "    feat1 = backbone(input1)\n",
    "    feat2 = backbone(input2)\n",
    "    feat3 = backbone(input3)\n",
    "    merged = layers.Concatenate()([feat1, feat2, feat3])\n",
    "    x = layers.Dense(512, activation='relu')(merged)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output = layers.Dense(len(label_cols)*3, activation='softmax')(x)\n",
    "    output = layers.Reshape((len(label_cols), 3))(output)\n",
    "    model = models.Model(inputs=[input1, input2, input3], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Train/Validation Split\n",
    "train_ids, val_ids = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['max_severity'])\n",
    "print(f\"Number of images in the training set: {len(train_ids)}\")\n",
    "print(f\"Number of images in the validation set: {len(val_ids)}\")\n",
    "\n",
    "\n",
    "train_gen = DataGenerator(train_ids)\n",
    "val_gen = DataGenerator(val_ids)\n",
    "\n",
    "# Build and train model\n",
    "model = build_mvcnn()\n",
    "\n",
    "# Callbacks\n",
    "#callbacks = [\n",
    "#    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),\n",
    "#    ModelCheckpoint('best_model_Adam.keras', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "#    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, min_lr=1e-6, verbose=1)\n",
    "#]\n",
    "\n",
    "# Train\n",
    "model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=EPOCHS,\n",
    "    #callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "model.save('mvc_MobileNetV2_no_aug_400k.h5')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8561470,
     "sourceId": 71549,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1727.059829,
   "end_time": "2025-06-21T04:51:32.792846",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-21T04:22:45.733017",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
